# RAG PDF Query System - Simple POC

A lightweight Retrieval-Augmented Generation (RAG) system for querying PDF documents using LangChain and Ollama with a simple Streamlit UI.

## ğŸŒŸ Features

ğŸ¤– **LangChain Integration**: Production-grade RAG using LangChain framework  
ğŸ“„ **PDF Processing**: Automatic PDF loading and intelligent chunking  
ğŸ” **Vector Search**: Semantic search using ChromaDB  
ğŸ’» **Simple UI**: Clean Streamlit interface  
ğŸ“š **Source Citations**: View relevant document sections  
âš™ï¸ **Model Switching**: Easy Ollama model selection  

## ğŸ“ Project Structure

```
RAG/
â”œâ”€â”€ main.py           # Application entry point
â”œâ”€â”€ app.py           # Streamlit UI
â”œâ”€â”€ rag_client.py    # Production RAG client (LangChain)
â”œâ”€â”€ requirements.txt # Dependencies
â”œâ”€â”€ data/           # Document storage and vector DB
â””â”€â”€ README.md       # This file
```

## ğŸš€ Quick Start

### Prerequisites

1. **Install Ollama**: Download from [ollama.ai](https://ollama.ai)
2. **Pull models**: 
   ```bash
   ollama pull llama2
   ollama pull nomic-embed-text  # For embeddings
   ```

### Installation & Run

```bash
# Install dependencies
pip install -r requirements.txt

# Start the app
python main.py
```

The app opens at `http://localhost:8501`

## ğŸ–¥ï¸ Usage

1. **Upload PDF**: Use the file uploader
2. **Wait for Processing**: Document gets chunked and embedded
3. **Ask Questions**: Type natural language questions
4. **View Results**: Get AI answers with source citations
5. **Switch Models**: Use sidebar to change Ollama models

## ğŸ¤– Supported Models

- `llama2` - Good general performance
- `llama3` - Latest with better reasoning  
- `mistral` - Fast and efficient
- `phi` - Lightweight for quick responses

## ğŸ”§ Configuration

The `RAGClient` class is production-ready and can be configured:

```python
from rag_client import RAGClient

# Initialize with custom settings
client = RAGClient(
    model_name="llama3",
    ollama_base_url="http://localhost:11434",
    persist_directory="./custom_db"
)

# Load document
result = client.load_pdf("document.pdf")

# Query
response = client.query("What is this about?")
```

## ğŸ› Troubleshooting

**Model not found**: Run `ollama pull llama2`  
**Embedding issues**: Run `ollama pull nomic-embed-text`  
**Connection error**: Ensure Ollama is running on port 11434  

## ğŸ“œ License

MIT License System

A production-ready Retrieval-Augmented Generation (RAG) system for querying PDF documents using Ollama and local language models with a modern Streamlit web interface.

## ğŸŒŸ Features

ğŸ¤– **Local AI Models**: Uses Ollama for private, local language model inference  
ğŸ“„ **PDF Processing**: Automatically extracts and chunks text from PDF documents  
ğŸ” **Smart Search**: Vector-based similarity search using sentence transformers  
ï¿½ **Modern Web UI**: Interactive Streamlit interface with real-time chat  
ğŸ“š **Source Citations**: Shows relevant document sections for each answer  
ğŸ”§ **Production Ready**: Structured codebase with logging, error handling, and configuration management  
âš™ï¸ **Configurable**: Adjustable chunk sizes, models, and similarity thresholds  
ğŸ“Š **Session Management**: Persistent chat history and document state  

## ğŸ“ Project Structure

```
RAG/
â”œâ”€â”€ main.py                 # Application entry point
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ __init__.py            # Package initialization
â”œâ”€â”€ data/                  # Data storage
â”‚   â””â”€â”€ chroma_db/         # Vector database storage
â”œâ”€â”€ logs/                  # Application logs
â”œâ”€â”€ src/                   # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core/              # Core RAG functionality
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ document_loader.py    # PDF loading and text extraction
â”‚   â”‚   â”œâ”€â”€ text_chunker.py       # Text splitting and chunking
â”‚   â”‚   â”œâ”€â”€ vector_database.py    # ChromaDB vector storage
â”‚   â”‚   â”œâ”€â”€ ollama_client.py      # Ollama integration
â”‚   â”‚   â””â”€â”€ rag_system.py         # Main RAG system coordination
â”‚   â”œâ”€â”€ ui/                # User interface
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ app.py               # Main Streamlit application
â”‚   â”‚   â””â”€â”€ components.py        # UI components and utilities
â”‚   â””â”€â”€ utils/             # Utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py            # Configuration management
â”‚       â””â”€â”€ logger.py            # Logging utilities
```

## ğŸš€ Quick Start

### Prerequisites

1. **Install Ollama**: Download from [ollama.ai](https://ollama.ai)
2. **Pull a model**: Run `ollama pull llama2` (or another model like `mistral`, `llama3`)
3. **Python 3.12+**: Ensure you have Python installed

### Installation

1. **Clone and navigate to the project**:
   ```bash
   cd RAG
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Start the application**:
   ```bash
   python main.py
   ```

The Streamlit web interface will automatically open in your browser at `http://localhost:8501`.

## ğŸ–¥ï¸ Using the Application

### 1. Upload a PDF
- Click "Choose a PDF file" in the upload section
- Select your PDF document
- Wait for the system to process and chunk the document

### 2. Configure Settings
Use the sidebar to adjust:
- **Model Selection**: Choose from available Ollama models
- **Retrieval Settings**: Number of results and similarity threshold
- **Document Info**: View loaded document details

### 3. Ask Questions
- Type your question in the query box
- Click "Ask" to get AI-generated responses
- View sources and citations for each answer

### 4. Search Content
- Use the search feature to find specific terms in your document
- Browse search results with similarity scores

### 5. Chat History
- View previous questions and answers in the chat history panel
- Expand any conversation to see sources and details

## âš™ï¸ Configuration

### Environment Variables

You can configure the system using environment variables:

```bash
# Model configuration
export OLLAMA_MODEL="llama3"
export EMBEDDING_MODEL="all-MiniLM-L6-v2"
export OLLAMA_HOST="http://localhost:11434"

# Chunking configuration
export CHUNK_SIZE="500"
export CHUNK_OVERLAP="50"

# Database configuration
export COLLECTION_NAME="my_documents"
export PERSIST_DIRECTORY="./data/my_chroma_db"
```

### Code Configuration

Modify `src/utils/config.py` to change default settings:

```python
@dataclass
class ModelConfig:
    default_model: str = "llama2"
    embedding_model: str = "all-MiniLM-L6-v2"
    max_context_length: int = 4000

@dataclass
class ChunkingConfig:
    chunk_size: int = 500
    chunk_overlap: int = 50
```

## ğŸ¤– Supported Models

Popular Ollama models you can use:
- `llama2` - Good balance of speed and quality
- `llama3` - Latest Llama model with improved performance  
- `mistral` - Fast and efficient
- `phi` - Lightweight model for quick responses
- `codellama` - Specialized for code-related content
- `neural-chat` - Optimized for conversational AI

## ğŸ”§ Development

### Running in Development Mode

```bash
# Demo mode (uses first PDF in directory)
python main.py --demo

# Direct Streamlit run
streamlit run src/ui/app.py
```

### Logging

Logs are automatically saved to the `logs/` directory with timestamps. You can adjust logging levels in `src/utils/logger.py`.

### Testing

```bash
# Run the demo to test basic functionality
python main.py --demo
```

## ğŸ› Troubleshooting

### Common Issues

1. **"Model not found"**:
   ```bash
   ollama list  # Check available models
   ollama pull llama2  # Pull a model
   ```

2. **"Ollama connection error"**:
   - Ensure Ollama is running: check `http://localhost:11434`
   - Restart Ollama service if needed

3. **"No text extracted from PDF"**:
   - Ensure PDF contains selectable text (not just images)
   - Try a different PDF or check file integrity

4. **Slow performance**:
   - Try a smaller/faster model like `phi`
   - Reduce chunk size or number of retrieved results
   - Use a faster embedding model

5. **Import errors**:
   - Ensure all dependencies are installed: `pip install -r requirements.txt`
   - Check Python version compatibility (3.12+)

### Performance Optimization

- **First run**: Initial setup downloads the embedding model (~80MB)
- **Model switching**: Larger models provide better quality but slower responses
- **Chunk optimization**: Balance between precision (smaller chunks) and context (larger chunks)
- **Hardware**: More RAM and CPU cores improve performance

## ğŸ“ API Usage

You can also use the RAG system programmatically:

```python
from src.core import RAGSystem

# Initialize the system
rag = RAGSystem("path/to/your/document.pdf")

# Load the document
result = rag.load_document()

# Query the document
response = rag.query("What is this document about?")
print(response['answer'])
```

## ğŸ”’ Privacy & Security

- **Local Processing**: All AI inference happens locally on your machine
- **No Data Transmission**: Your documents never leave your computer  
- **Private Models**: Use your own Ollama models without external API calls
- **Secure Storage**: Vector embeddings stored locally in ChromaDB

## ğŸ“œ License

This project is open source and available under the MIT License.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“ Support

If you encounter issues:
1. Check the troubleshooting section above
2. Review the logs in the `logs/` directory
3. Ensure Ollama is properly installed and running
4. Verify all dependencies are installed correctly

---

**Built with â¤ï¸ using Ollama, Streamlit, ChromaDB, and Sentence Transformers**